---
title: "Project1_SpookyData_text_minging"
UNI: kl3016
Name: Kai Li
output:
  pdf_document: default
  html_notebook: default

---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 
Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.


#Section 0: Package Used and Load
```{r}
#R package used in project
library(ggplot2)
library(dplyr)
library(tibble)
library(tidyr)
library(stringr)
library(tidytext)
library(topicmodels)
library(wordcloud)
library(ggridges)
library(textstem)
library(compare)
library(NMF)
library(devtools)
library(pca3d)


```

##Multiple plot function
```{r}
# Refer to http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  require(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```


#Section 1: Data cleaning and visualation
Read the data and process sentences to one word per row. 
Also, stream words by use the function
Try to split sentences into two or three worlds per row to explore some meaningful features.

#Tokenization & Streaming
```{r}
# read the data
spooky <- read.csv('../data/spooky.csv', as.is = TRUE)

sum(is.na(spooky))
spooky$author <- as.factor(spooky$author)


# Make a table with one word per row and remove `stop words` (i.e. the common words).
spooky_wrd <- unnest_tokens(spooky, word, text)
spooky_wrd <- anti_join(spooky_wrd, stop_words, by = "word")

#Stream words
spooky_wrd1 <- spooky_wrd
spooky_wrd1$stem <- stem_words(spooky_wrd1$word, language = "porter")


```
Streaming helps combine some words with same meaning.However,by oberserving the data set after streaming, there are some problems exist. For example, "happy" becomes "happi" after streaming. This would have negative impace on some analysis especially the sentimental analysis.



# Make a table with two or three words per row and remove 'stop words'
```{r}
# set n=2 to split words, so two words are considered as one cell in the data set.
spooky_wrd2 <- unnest_tokens(spooky, word, text, token = "skip_ngrams", n = 2)
spooky_wrd2 <- anti_join(spooky_wrd2, stop_words, by = "word")

# set n=2 
spooky_wrd3 <- unnest_tokens(spooky, word, text, token = "skip_ngrams", n = 3)
spooky_wrd3 <- anti_join(spooky_wrd3, stop_words, by = "word")
```


#Worldcloud for single words with and without streaming
```{r}
# explore words without streaming frequency 
x <- count(group_by(spooky_wrd, word))
words <- x$word
freqs <- x$n

png("../figs/Wordcloud_all.png")
wordcloud(words, freqs, max.words = 50, color = c("purple4", "red4", "black"))
dev.off()

```


```{r}
# explore words with streaming frequency 
x1 <- count(group_by(spooky_wrd1, stem))
words1 <- x1$stem
freqs1 <- x1$n


#png("../figs/Wordcloud_streamingWords.png")
wordcloud(words1, freqs1, max.words = 50, color = c("purple4", "red4", "black"))
#dev.off()
```
Comparing two wordclouds, after streaming the frequency of certain words increaing such as "hoop" and "pass". Some seious problems are happened such as "eye" becoming "ey", "y" at the end becoming "i".


#Compare differences between streaming and non-streaming words in top 50 frequently.
```{r}
x3 <- x[order(-x$n),]
x4 <- x1[order(-x1$n),]

# compare the top 50 frequencies words between streaming and unstreaming data sets
comparison <- cbind(x3$word[1:50], x3$n[1:50], x4$stem[1:50], x4$n[1:50])
comparison <- data.frame(comparison)
names(comparison) <- c("word non-streaming","frequency1", "word streaming", "frequency2")
comparison
```
This table of comparison makes it clear that the changes of frequencies between streaming words list and non-streaming words list. 


#Plot the more frequnt streaming words used each author
```{r}
# Counts number of times each author used each word.
author_words <- count(group_by(spooky_wrd1, stem, author))

# Counts number of times each word was used.
all_words    <- rename(count(group_by(spooky_wrd1, stem)), all = n)

author_words <- left_join(author_words, all_words, by = "stem")
author_words <- arrange(author_words, desc(all))
author_words <- ungroup(head(author_words, 81))
  
f1 <- ggplot(author_words) +
  geom_col(aes(reorder(stem, all, FUN = min), n, fill = author)) +
  xlab(NULL) +
  coord_flip() +
  facet_wrap(~ author) +
  theme(legend.position = "none")+
  ggtitle("Most frequent words used by Author (streaming)")

# Counts number of times each author used each word.
author_words <- count(group_by(spooky_wrd, word, author))

# Counts number of times each word was used.
all_words    <- rename(count(group_by(spooky_wrd, word)), all = n)

author_words <- left_join(author_words, all_words, by = "word")
author_words <- arrange(author_words, desc(all))
author_words <- ungroup(head(author_words, 81))
  
f2 <- ggplot(author_words) +
  geom_col(aes(reorder(word, all, FUN = min), n, fill = author)) +
  xlab(NULL) +
  coord_flip() +
  facet_wrap(~ author) +
  theme(legend.position = "none")+
  ggtitle("Most frequent words used by Author (non-streaming)")

png("../figs/Most frequent words used by Author (non-streaming).png")
f2
dev.off()


png("../figs/Most frequent words used by Author (streaming).png")
f1
dev.off()
```
It is a better visualization of words frequencies for differences with or without streaming.


#Section 2: Converting punctuation mark features to numerical features and Run Clustering
#Special features
```{r}

SPspooky <- spooky
SPspooky$comma      <- 0
SPspooky$semicolon  <- 0
SPspooky$quote      <- 0

SPspooky$comma[1:19579]        <- as.numeric(str_count(SPspooky$text[1:19579], ','))
SPspooky$semicolon[1:19579]    <- as.numeric(str_count(SPspooky$text[1:19579], ';'))
SPspooky$quote[1:19579]        <- as.numeric(str_count(SPspooky$text[1:19579], '"'))
SPspooky$sen_length <- str_length(SPspooky$text)


p1 <- ggplot(SPspooky) +
      geom_bar(aes(author, fill = author)) +
      theme(legend.position = "none")


p2 <- ggplot(SPspooky) +
      geom_density_ridges(aes(sen_length, author, fill = author)) +
      scale_x_log10() +
      theme(legend.position = "none") +
      labs(x = "Sentence length [# characters]")

png("../figs/Author words used and sentences length.png")
multiplot(p1, p2, cols =2)
dev.off()




p3 <- ggplot(SPspooky,aes(x= comma, fill = author, )) +
      geom_density( bw = 0.1, alpha = 0.3) +
      scale_x_log10() +
      theme(legend.position = "none") +
      labs(x = "Comma Used")

p4 <- ggplot(SPspooky) +
      geom_density(aes(semicolon, fill = author), bw = 0.1, alpha = 0.3) +
      scale_x_log10() +
      theme(legend.position = "none") +
      labs(x = "Semicolon Used")

p5 <- ggplot(SPspooky) +
      geom_density(aes(quote, fill = author), bw = 0.1, alpha = 0.3) +
      scale_x_log10() +
      theme(legend.position = "none") +
      labs(x = "Quote Used")


png("../figs/Punctuation marks frequences for each author.png")
multiplot(p3,p4,p5, cols =3)
dev.off()
 
```


#Classification Based on fatures explored above
```{r}

pca = prcomp(SPspooky[,4:7], scale. = TRUE)
p <- pca2d(pca, group=SPspooky$author, legend="bottomright",biplot=TRUE, biplot.vars=4)

png("../figs/PCA Clastering Plot.png")
pca2d(pca, group=SPspooky$author, legend="bottomright",biplot=TRUE, biplot.vars=4)
dev.off()


```
Explore festure of punctuation marks such as ",", ";" and "'". Try to do classification by using PCA methods. However, the plots shows comma and semicolon are at the same direction with sentence length. and quote are relatively less related to the other three.
It is reasonable. When the length of sentences becomes longer, it tends to have more comma or semicolon. Thus, it is hard to use the length of sentences, comma, semicolon to predict the author. However, at the quote direction, there are more yellow dots concentrated. It means EAP tends to use more quote than ohter two authors. In other words, EAP would have more dialogs than the other two authors.


#Section 3: Sentiment Analysis
#Sentimental Analysis in Word Level
Do not use the streamed data for the sentimental analysis, because some words are twisted. For example, after streaming happy would change to happi. Such change would not return correct results.

```{r}
# Keep words that have been classified within the NRC lexicon.
nrc <- get_sentiments('nrc')
sentiments  <- inner_join(spooky_wrd, get_sentiments('nrc'), by = "word")


count(sentiments, sentiment)
count(sentiments, author, sentiment)

png("../figs/Sentimental analysis.png")
ggplot(count(sentiments, sentiment)) + 
  geom_col(aes(sentiment, n, fill = sentiment))
dev.off()

png("../figs//Sentimental analysis for each author.png")
ggplot(count(sentiments, author, sentiment)) + 
  geom_col(aes(sentiment, n, fill = sentiment)) + 
  facet_wrap(~ author) +
  coord_flip() +
  theme(legend.position = "none")
dev.off()

```

#Sentiment Analysis in Sentence Level
```{r}
#use the dictionary nrc. Seperate its sentiment columns into two categories: positive and negative
posWords <- c("anticipation","joy","positve","surprise","trust")
negWords <- c("anger","disgust","fear","negative","sadness")


#set 1 for positive category, and -1 for negative category.
SentimentJudge <-  for (i in 1:nrow(nrc)){
    if (nrc$sentiment[i] %in% posWords) {
      nrc$score[i] <- 1
      }
  else{
    nrc$score[i] <- -1
      }
  }


nrc$lengths<-unlist(lapply(nrc$word, nchar))
nrc<-nrc[ order(-nrc[,4]),]

#write the function to compare words between sentences and dictory, and then calculate the socre for each sentence
scoreSentence <- function(sentence){
  score<-0
  for(i in 1:nrow(nrc)){
    count<- length(grep(nrc[i,1],sentence))
    if(count){
      score<-score + (count * nrc[i,3])
      sentence<-sub(nrc[i,1],'',sentence)
    }
  }
  score
}

#pick a small set
small_sample <- spooky[1:100,]
SScore<- unlist(lapply(small_sample$text, scoreSentence))
small_sample <- cbind(small_sample, SScore)

#If the score of sentence >0, it consider as the positive sentiment. If the score of sentence <0, it consider as the negative sentiment. 

png("../figs/Sentimental analysis at sentence level.png")
qplot( author,SScore, data=small_sample, geom=c("boxplot", "jitter"), 
   fill=author, main="Sentence Sentimental Scores for the first 100 Samples",
   xlab="", ylab="Sentence Sentimental Score")
dev.off()

```
Sentimental analysis in sentecn level:
It tooks lots of time to run the program, so pick a small set as the sample to run the sentimental analysis at sentence level. By observing the box plot, it shows MWS has relavtivly lower scores than other two authors. Majority of dots are located below the 0 line. It means the majority of the three authors' sentences are considered as the negative attitude. It mattches their writting style. Also, MWS have lower scores can be shown by the words sentimental analysis that MWS used more negative words. 

problem:
1. it is hard to determin such large set of sentecs
2. Some worlds present different semtimental such lovely. Lovely can represent both positive and negative.
3. It is not accurate to determin levels of sentiment




#Topic Modleing by using LDA and NMF
#LDA 
In the text mining, we are interested in the unsuperavised classification for the document. We are looking for the natural groups of these sentences. 
The Latent Dirichlet Allocation is a popular method. In this model, it creats a mixture of topics and each topic as a mixture of words. For example, in a two-topic model, text 1 is 90% topic An and 10% topic B. Use special words appeared in the text to say the posible topoc for the text.
```{r}
sent_wrd_freqs <- count(spooky_wrd, id, word)
head(sent_wrd_freqs)

# Creates a DTM matrix
spooky_wrd_tm <- cast_dtm(sent_wrd_freqs, id, word, n)
spooky_wrd_tm
length(unique(spooky_wrd$id))
length(unique(spooky_wrd$word))

```

# Find the best fit of number for the Topic Modeling
```{r}
library("topicmodels")
library("ldatuning")


result <- FindTopicsNumber(
  dtm,
  topics = seq(from = 2, to = 15, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE
)

FindTopicsNumber_plot(FindTopicsNumber)
```



```{r}

spooky_wrd_lda <- LDA(spooky_wrd_tm, k =10, control = list(seed = 1234))
spooky_wrd_topics <- tidy(spooky_wrd_lda, matrix = "beta")
spooky_wrd_topics


# Grab the top five words for each topic.
spooky_wrd_topics_5 <- ungroup(top_n(group_by(spooky_wrd_topics, topic), 5, beta))
spooky_wrd_topics_5 <- arrange(spooky_wrd_topics_5, topic, -beta)
spooky_wrd_topics_5 <- mutate(spooky_wrd_topics_5, term = reorder(term, beta))

ggplot(spooky_wrd_topics_5) +
  geom_col(aes(term, beta, fill = factor(topic)), show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol = 4) +
  coord_flip()



```


# TF-IDF

TF stands for term frequency or how often a word appears in a text and it is what is studied above in the word cloud. IDF stands for inverse document frequncy, and it is a way to pay more attention to words that are rare within the entire set of text data that is more sophisticated than simply removing stop words.  Multiplying these two values together calculates a term's tf-idf, which is the frequency of a term adjusted for how rarely it is used. 

We'll use tf-idf as a heuristic index to indicate how frequently a certain author uses a word relative to the frequency that all the authors use the word.  Therefore we will find words that are characteristic for a specific author, a good thing to have if we are interested in solving the author identification problem.

```{r}
frequency <- count(spooky_wrd, author, word)
tf_idf    <- bind_tf_idf(frequency, word, author, n)
head(tf_idf)
tail(tf_idf)

tf_idf    <- arrange(tf_idf, desc(tf_idf))
tf_idf    <- mutate(tf_idf, word = factor(word, levels = rev(unique(word))))

# Grab the top thirty tf_idf scores in all the words 
tf_idf_30 <- top_n(tf_idf, 30, tf_idf)

ggplot(tf_idf_30) +
  geom_col(aes(word, tf_idf, fill = author)) +
  labs(x = NULL, y = "TF-IDF values") +
  theme(legend.position = "top", axis.text.x  = element_text(angle=45, hjust=1, vjust=0.9))

```

Note that in the above, many of the words recognized by their tf-idf scores are names.  This makes sense -- if we see text referencing Raymond, Idris, or Perdita, we know almost for sure that MWS is the author.  But some non-names stand out.  EAP often uses "monsieur" and "jupiter" while HPL uses the words "bearded" and "attic" more frequently than the others.  We can also look at the most characteristic terms per author.

```{r}
# Grab the top twenty tf_idf scores in all the words for each author
tf_idf <- ungroup(top_n(group_by(tf_idf, author), 20, tf_idf))
  
ggplot(tf_idf) +
  geom_col(aes(word, tf_idf, fill = author)) +
  labs(x = NULL, y = "tf-idf") +
  theme(legend.position = "none") +
  facet_wrap(~ author, ncol = 3, scales = "free") +
  coord_flip() +
  labs(y = "TF-IDF values")
```





