# Counts number of times each word was used.
all_words    <- rename(count(group_by(spooky_wrd1, stem)), all = n)
author_words <- left_join(author_words, all_words, by = "stem")
author_words <- arrange(author_words, desc(all))
author_words <- ungroup(head(author_words, 81))
f1 <- ggplot(author_words) +
geom_col(aes(reorder(stem, all, FUN = min), n, fill = author)) +
xlab(NULL) +
coord_flip() +
facet_wrap(~ author) +
theme(legend.position = "none")+
ggtitle("Most frequent words used by Author (streaming)")
# Counts number of times each author used each word.
author_words <- count(group_by(spooky_wrd, word, author))
# Counts number of times each word was used.
all_words    <- rename(count(group_by(spooky_wrd, word)), all = n)
author_words <- left_join(author_words, all_words, by = "word")
author_words <- arrange(author_words, desc(all))
author_words <- ungroup(head(author_words, 81))
f2 <- ggplot(author_words) +
geom_col(aes(reorder(word, all, FUN = min), n, fill = author)) +
xlab(NULL) +
coord_flip() +
facet_wrap(~ author) +
theme(legend.position = "none")+
ggtitle("Most frequent words used by Author (non-streaming)")
#png("../figs/Most frequent words used by Author (non-streaming).png")
f2
#dev.off()
#png("../figs/Most frequent words used by Author (streaming).png")
f1
#dev.off()
# Initialize the punctuation mark features
SPspooky <- spooky
SPspooky$comma      <- 0
SPspooky$semicolon  <- 0
SPspooky$quote      <- 0
# Convert string features into numerical features
SPspooky$comma[1:19579]        <- as.numeric(str_count(SPspooky$text[1:19579], ','))
SPspooky$semicolon[1:19579]    <- as.numeric(str_count(SPspooky$text[1:19579], ';'))
SPspooky$quote[1:19579]        <- as.numeric(str_count(SPspooky$text[1:19579], '"'))
SPspooky$sen_length <- str_length(SPspooky$text)
# Plot the length of sentences
p1 <- ggplot(SPspooky) +
geom_bar(aes(author, fill = author)) +
theme(legend.position = "none")
p2 <- ggplot(SPspooky) +
geom_density_ridges(aes(sen_length, author, fill = author)) +
scale_x_log10() +
theme(legend.position = "none") +
labs(x = "Sentence length [# characters]")
#png("../figs/Author words used and sentences length.png")
multiplot(p1, p2, cols =2)
#dev.off()
# Plot punctuation features for each author
p3 <- ggplot(SPspooky,aes(x= comma, fill = author, )) +
geom_density( bw = 0.1, alpha = 0.3) +
scale_x_log10() +
theme(legend.position = "none") +
labs(x = "Comma Used")
p4 <- ggplot(SPspooky) +
geom_density(aes(semicolon, fill = author), bw = 0.1, alpha = 0.3) +
scale_x_log10() +
theme(legend.position = "none") +
labs(x = "Semicolon Used")
p5 <- ggplot(SPspooky) +
geom_density(aes(quote, fill = author), bw = 0.1, alpha = 0.3) +
scale_x_log10() +
theme(legend.position = "none") +
labs(x = "Quote Used")
#png("../figs/Punctuation marks frequences for each author.png")
multiplot(p3,p4,p5, cols =3)
#dev.off()
legend("topright", inset=.05, title="Number of Cylinders",
c("4","6","8"), fill=SPspooky$author, horiz=TRUE)
legend("topright", inset=.05, title="Number of Cylinders",
c("4","6","8"), fill=SPspooky$author, horiz=TRUE)
#png("../figs/Punctuation marks frequences for each author.png")
multiplot(p3,p4,p5, cols =3)
legend("topright", inset=.05, title="Number of Cylinders",
c("4","6","8"), fill=SPspooky$author, horiz=TRUE)
#png("../figs/Punctuation marks frequences for each author.png")
multiplot(p3,p4,p5, cols =3)
#png("../figs/Punctuation marks frequences for each author.png")
multiplot(p3,p4,p5, cols =3)
legend("topright", inset=.05, title="Number of Cylinders",
c("4","6","8"), fill=SPspooky$author, horiz=TRUE)
p5 <- ggplot(SPspooky) +
geom_density(aes(quote, fill = author), bw = 0.1, alpha = 0.3) +
scale_x_log10() +
theme(legend.position = "none") +
theme(legend.position="none")+
labs(x = "Quote Used")
#png("../figs/Punctuation marks frequences for each author.png")
multiplot(p3,p4,p5, cols =3)
p5 <- ggplot(SPspooky) +
geom_density(aes(quote, fill = author), bw = 0.1, alpha = 0.3) +
scale_x_log10() +
theme(legend.position = "none") +
theme(legend.position="right")+
labs(x = "Quote Used")
#png("../figs/Punctuation marks frequences for each author.png")
multiplot(p3,p4,p5, cols =3)
p5 <- ggplot(SPspooky) +
geom_density(aes(quote, fill = author), bw = 0.1, alpha = 0.3) +
scale_x_log10() +
theme(legend.position="top")+
labs(x = "Quote Used")
#png("../figs/Punctuation marks frequences for each author.png")
multiplot(p3,p4,p5, cols =3)
# Initialize the punctuation mark features
SPspooky <- spooky
SPspooky$comma      <- 0
SPspooky$semicolon  <- 0
SPspooky$quote      <- 0
# Convert string features into numerical features
SPspooky$comma[1:19579]        <- as.numeric(str_count(SPspooky$text[1:19579], ','))
SPspooky$semicolon[1:19579]    <- as.numeric(str_count(SPspooky$text[1:19579], ';'))
SPspooky$quote[1:19579]        <- as.numeric(str_count(SPspooky$text[1:19579], '"'))
SPspooky$sen_length <- str_length(SPspooky$text)
# Plot the length of sentences
p1 <- ggplot(SPspooky) +
geom_bar(aes(author, fill = author)) +
theme(legend.position = "none")
p2 <- ggplot(SPspooky) +
geom_density_ridges(aes(sen_length, author, fill = author)) +
scale_x_log10() +
theme(legend.position = "none") +
labs(x = "Sentence length [# characters]")
#png("../figs/Author words used and sentences length.png")
multiplot(p1, p2, cols =2)
#dev.off()
# Plot punctuation features for each author
p3 <- ggplot(SPspooky,aes(x= comma, fill = author, )) +
geom_density( bw = 0.1, alpha = 0.3) +
scale_x_log10() +
theme(legend.position = "top") +
labs(x = "Comma Used")
p4 <- ggplot(SPspooky) +
geom_density(aes(semicolon, fill = author), bw = 0.1, alpha = 0.3) +
scale_x_log10() +
theme(legend.position = "top") +
labs(x = "Semicolon Used")
p5 <- ggplot(SPspooky) +
geom_density(aes(quote, fill = author), bw = 0.1, alpha = 0.3) +
scale_x_log10() +
theme(legend.position="top")+
labs(x = "Quote Used")
#png("../figs/Punctuation marks frequences for each author.png")
multiplot(p3,p4,p5, cols =3)
#dev.off()
# Use PCA method to do classification
pca = prcomp(SPspooky[,4:7], scale. = TRUE)
p <- pca2d(pca, group=SPspooky$author, legend="bottomright",biplot=TRUE, biplot.vars=4)
#png("../figs/PCA Clastering Plot.png")
pca2d(pca, group=SPspooky$author, legend="bottomright",biplot=TRUE, biplot.vars=4)
#dev.off()
# Use PCA method to do classification
pca = prcomp(SPspooky[,4:7], scale. = TRUE)
p <- pca2d(pca, group=SPspooky$author, legend="bottomright",biplot=TRUE, biplot.vars=4)
#png("../figs/PCA Clastering Plot.png")
#pca2d(pca, group=SPspooky$author, legend="bottomright",biplot=TRUE, biplot.vars=4)
#dev.off()
# Keep words that have been classified into the NRC lexicon.
nrc <- get_sentiments('nrc')
sentiments  <- inner_join(spooky_wrd, get_sentiments('nrc'), by = "word")
count(sentiments, sentiment)
count(sentiments, author, sentiment)
#png("../figs/Sentimental analysis.png")
ggplot(count(sentiments, sentiment)) +
geom_col(aes(sentiment, n, fill = sentiment))
#dev.off()
#png("../figs//Sentimental analysis for each author.png")
ggplot(count(sentiments, author, sentiment)) +
geom_col(aes(sentiment, n, fill = sentiment)) +
facet_wrap(~ author) +
coord_flip() +
theme(legend.position = "none")
#dev.off()
# Use the dictionary nrc. Separate its sentiment columns into two categories: positive and negative
posWords <- c("anticipation","joy","positve","surprise","trust")
negWords <- c("anger","disgust","fear","negative","sadness")
# Set 1 for the positive category, and -1 for the negative category
SentimentJudge <-  for (i in 1:nrow(nrc)){
if (nrc$sentiment[i] %in% posWords) {
nrc$score[i] <- 1
}
else{
nrc$score[i] <- -1
}
}
nrc$lengths<-unlist(lapply(nrc$word, nchar))
nrc<-nrc[ order(-nrc[,4]),]
# Write the function to compare words between sentences and directory, and then calculate the score for each sentence
scoreSentence <- function(sentence){
score<-0
for(i in 1:nrow(nrc)){
count<- length(grep(nrc[i,1],sentence))
if(count){
score<-score + (count * nrc[i,3])
sentence<-sub(nrc[i,1],'',sentence)
}
}
score
}
# Pick a small set
small_sample <- spooky[1:100,]
SScore<- unlist(lapply(small_sample$text, scoreSentence))
small_sample <- cbind(small_sample, SScore)
# If the score of sentence >0, it considers as the positive sentiment. If the score of sentence <0, it considers as the negative sentiment.
#png("../figs/Sentimental analysis at sentence level.png")
qplot( author,SScore, data=small_sample, geom=c("boxplot", "jitter"),
fill=author, main="Sentence Sentimental Scores for the first 100 Samples",
xlab="", ylab="Sentence Sentimental Score")
#dev.off()
# Use the dictionary nrc. Separate its sentiment columns into two categories: positive and negative
posWords <- c("anticipation","joy","positve","surprise","trust")
negWords <- c("anger","disgust","fear","negative","sadness")
# Set 1 for the positive category, and -1 for the negative category
SentimentJudge <-  for (i in 1:nrow(nrc)){
if (nrc$sentiment[i] %in% posWords) {
nrc$score[i] <- 1
}
else{
nrc$score[i] <- -1
}
}
nrc$lengths<-unlist(lapply(nrc$word, nchar))
nrc<-nrc[ order(-nrc[,4]),]
# Write the function to compare words between sentences and directory, and then calculate the score for each sentence
scoreSentence <- function(sentence){
score<-0
for(i in 1:nrow(nrc)){
count<- length(grep(nrc[i,1],sentence))
if(count){
score<-score + (count * nrc[i,3])
sentence<-sub(nrc[i,1],'',sentence)
}
}
}
# Pick a small set
small_sample <- spooky[1:100,]
SScore<- unlist(lapply(small_sample$text, scoreSentence))
small_sample <- cbind(small_sample, SScore)
# Use the dictionary nrc. Separate its sentiment columns into two categories: positive and negative
posWords <- c("anticipation","joy","positve","surprise","trust")
negWords <- c("anger","disgust","fear","negative","sadness")
# Set 1 for the positive category, and -1 for the negative category
SentimentJudge <-  for (i in 1:nrow(nrc)){
if (nrc$sentiment[i] %in% posWords) {
nrc$score[i] <- 1
}
else{
nrc$score[i] <- -1
}
}
nrc$lengths<-unlist(lapply(nrc$word, nchar))
nrc<-nrc[ order(-nrc[,4]),]
# Write the function to compare words between sentences and directory, and then calculate the score for each sentence
scoreSentence <- function(sentence){
sentence$score<-0
for(i in 1:nrow(nrc)){
count<- length(grep(nrc[i,1],sentence))
if(count){
score<-score + (count * nrc[i,3])
sentence<-sub(nrc[i,1],'',sentence)
}
}
score
}
# Pick a small set
small_sample <- spooky[1:100,]
SScore<- unlist(lapply(small_sample$text, scoreSentence))
# Use the dictionary nrc. Separate its sentiment columns into two categories: positive and negative
posWords <- c("anticipation","joy","positve","surprise","trust")
negWords <- c("anger","disgust","fear","negative","sadness")
# Set 1 for the positive category, and -1 for the negative category
SentimentJudge <-  for (i in 1:nrow(nrc)){
if (nrc$sentiment[i] %in% posWords) {
nrc$score[i] <- 1
}
else{
nrc$score[i] <- -1
}
}
nrc$lengths<-unlist(lapply(nrc$word, nchar))
nrc<-nrc[ order(-nrc[,4]),]
# Write the function to compare words between sentences and directory, and then calculate the score for each sentence
scoreSentence <- function(sentence){
sentence$score<-0
for(i in 1:nrow(nrc)){
count<- length(grep(nrc[i,1],sentence))
if(count){
score<-score + (count * nrc[i,3])
sentence<-sub(nrc[i,1],'',sentence)
}
}
score
}
# Pick a small set
small_sample <- spooky[1:10,]
SScore<- unlist(lapply(small_sample$text, scoreSentence))
# Use the dictionary nrc. Separate its sentiment columns into two categories: positive and negative
posWords <- c("anticipation","joy","positve","surprise","trust")
negWords <- c("anger","disgust","fear","negative","sadness")
# Set 1 for the positive category, and -1 for the negative category
SentimentJudge <-  for (i in 1:nrow(nrc)){
if (nrc$sentiment[i] %in% posWords) {
nrc$score[i] <- 1
}
else{
nrc$score[i] <- -1
}
}
nrc$lengths<-unlist(lapply(nrc$word, nchar))
nrc<-nrc[ order(-nrc[,4]),]
# Write the function to compare words between sentences and directory, and then calculate the score for each sentence
scoreSentence <- function(sentence){
score<-0
for(i in 1:nrow(nrc)){
count<- length(grep(nrc[i,1],sentence))
if(count){
score<-score + (count * nrc[i,3])
sentence<-sub(nrc[i,1],'',sentence)
}
}
score
}
# Pick a small set
small_sample <- spooky[1:10,]
SScore<- unlist(lapply(small_sample$text, scoreSentence))
small_sample <- cbind(small_sample, SScore)
# If the score of sentence >0, it considers as the positive sentiment. If the score of sentence <0, it considers as the negative sentiment.
#png("../figs/Sentimental analysis at sentence level.png")
qplot( author,SScore, data=small_sample, geom=c("boxplot", "jitter"),
fill=author, main="Sentence Sentimental Scores for the first 100 Samples",
xlab="", ylab="Sentence Sentimental Score")
#dev.off()
# Use the dictionary nrc. Separate its sentiment columns into two categories: positive and negative
posWords <- c("anticipation","joy","positve","surprise","trust")
negWords <- c("anger","disgust","fear","negative","sadness")
# Set 1 for the positive category, and -1 for the negative category
SentimentJudge <-  for (i in 1:nrow(nrc)){
if (nrc$sentiment[i] %in% posWords) {
nrc$score[i] <- 1
}
else{
nrc$score[i] <- -1
}
}
nrc$lengths<-unlist(lapply(nrc$word, nchar))
nrc<-nrc[ order(-nrc[,4]),]
# Write the function to compare words between sentences and directory, and then calculate the score for each sentence
scoreSentence <- function(sentence){
score<-0
for(i in 1:nrow(nrc)){
count<- length(grep(nrc[i,1],sentence))
if(count){
score<-score + (count * nrc[i,3])
sentence<-sub(nrc[i,1],'',sentence)
}
}
score
}
# Pick a small set
small_sample <- spooky[1:100,]
SScore<- unlist(lapply(small_sample$text, scoreSentence))
small_sample <- cbind(small_sample, SScore)
# If the score of sentence >0, it considers as the positive sentiment. If the score of sentence <0, it considers as the negative sentiment.
#png("../figs/Sentimental analysis at sentence level.png")
qplot( author,SScore, data=small_sample, geom=c("boxplot", "jitter"),
fill=author, main="Sentence Sentimental Scores for the first 100 Samples",
xlab="", ylab="Sentence Sentimental Score")
#dev.off()
# Initialize the punctuation mark features
SPspooky <- spooky
SPspooky$comma      <- 0
SPspooky$semicolon  <- 0
SPspooky$quote      <- 0
# Convert string features into numerical features
SPspooky$comma[1:19579]        <- as.numeric(str_count(SPspooky$text[1:19579], ','))
SPspooky$semicolon[1:19579]    <- as.numeric(str_count(SPspooky$text[1:19579], ';'))
SPspooky$quote[1:19579]        <- as.numeric(str_count(SPspooky$text[1:19579], '"'))
SPspooky$sen_length <- str_length(SPspooky$text)
# Plot the length of sentences
p1 <- ggplot(SPspooky) +
geom_bar(aes(author, fill = author)) +
theme(legend.position = "left")
p2 <- ggplot(SPspooky) +
geom_density_ridges(aes(sen_length, author, fill = author)) +
scale_x_log10() +
theme(legend.position = "right") +
labs(x = "Sentence length [# characters]")
#png("../figs/Author words used and sentences length.png")
multiplot(p1, p2, cols =2)
#dev.off()
# Plot punctuation features for each author
p3 <- ggplot(SPspooky,aes(x= comma, fill = author, )) +
geom_density( bw = 0.1, alpha = 0.3) +
scale_x_log10() +
theme(legend.position = "top") +
labs(x = "Comma Used")
p4 <- ggplot(SPspooky) +
geom_density(aes(semicolon, fill = author), bw = 0.1, alpha = 0.3) +
scale_x_log10() +
theme(legend.position = "top") +
labs(x = "Semicolon Used")
p5 <- ggplot(SPspooky) +
geom_density(aes(quote, fill = author), bw = 0.1, alpha = 0.3) +
scale_x_log10() +
theme(legend.position="top")+
labs(x = "Quote Used")
#png("../figs/Punctuation marks frequences for each author.png")
multiplot(p3,p4,p5, cols =3)
#dev.off()
# Use PCA method to do classification
pca = prcomp(SPspooky[,4:7], scale. = TRUE)
p <- pca2d(pca, group=SPspooky$author, legend="bottomright",biplot=TRUE, biplot.vars=4, title = "PCA plot for punctuation marks and length of sentences")
#png("../figs/PCA Clastering Plot.png")
#pca2d(pca, group=SPspooky$author, legend="bottomright",biplot=TRUE, biplot.vars=4)
#dev.off()
# Use PCA method to do classification
pca = prcomp(SPspooky[,4:7], scale. = TRUE)
p <- pca2d(pca, group=SPspooky$author, legend="bottomright",biplot=TRUE, biplot.vars=4, title = PCA plot for punctuation marks and length of sentences)
# Use PCA method to do classification
pca = prcomp(SPspooky[,4:7], scale. = TRUE)
p <- pca2d(pca, group=SPspooky$author, legend="bottomright",biplot=TRUE, biplot.vars=4, title = 'PCA plot for punctuation marks and length of sentences')
#png("../figs/PCA Clastering Plot.png")
#pca2d(pca, group=SPspooky$author, legend="bottomright",biplot=TRUE, biplot.vars=4)
#dev.off()
# Initialize the punctuation mark features
SPspooky <- spooky
SPspooky$comma      <- 0
SPspooky$semicolon  <- 0
SPspooky$quote      <- 0
# Convert string features into numerical features
SPspooky$comma[1:19579]        <- as.numeric(str_count(SPspooky$text[1:19579], ','))
SPspooky$semicolon[1:19579]    <- as.numeric(str_count(SPspooky$text[1:19579], ';'))
SPspooky$quote[1:19579]        <- as.numeric(str_count(SPspooky$text[1:19579], '"'))
SPspooky$sen_length <- str_length(SPspooky$text)
# Plot the length of sentences
p1 <- ggplot(SPspooky) +
geom_bar(aes(author, fill = author)) +
theme(legend.position = "left")+
ggtitle("Words used for each author")
p2 <- ggplot(SPspooky) +
geom_density_ridges(aes(sen_length, author, fill = author)) +
scale_x_log10() +
theme(legend.position = "right") +
labs(x = "Sentence length [# characters]")+
ggtitle("Sentences length for each author")
#png("../figs/Author words used and sentences length.png")
multiplot(p1, p2, cols =2)
#dev.off()
# Plot punctuation features for each author
p3 <- ggplot(SPspooky,aes(x= comma, fill = author, )) +
geom_density( bw = 0.1, alpha = 0.3) +
scale_x_log10() +
theme(legend.position = "top") +
labs(x = "Comma Used")+
ggtitle("Comma used for each author")
p4 <- ggplot(SPspooky) +
geom_density(aes(semicolon, fill = author), bw = 0.1, alpha = 0.3) +
scale_x_log10() +
theme(legend.position = "top") +
labs(x = "Semicolon Used")+
ggtitle("Semicolon used for each author")
p5 <- ggplot(SPspooky) +
geom_density(aes(quote, fill = author), bw = 0.1, alpha = 0.3) +
scale_x_log10() +
theme(legend.position="top")+
labs(x = "Quote Used")+
ggtitle("Quote used for each author")
#png("../figs/Punctuation marks frequences for each author.png")
multiplot(p3,p4,p5, cols =3)
#dev.off()
# Keep words that have been classified into the NRC lexicon.
nrc <- get_sentiments('nrc')
sentiments  <- inner_join(spooky_wrd, get_sentiments('nrc'), by = "word")
count(sentiments, sentiment)
count(sentiments, author, sentiment)
#png("../figs/Sentimental analysis.png")
ggplot(count(sentiments, sentiment)) +
geom_col(aes(sentiment, n, fill = sentiment))+
ggtitle("Sentimental features for the whole data")
#dev.off()
#png("../figs//Sentimental analysis for each author.png")
ggplot(count(sentiments, author, sentiment)) +
geom_col(aes(sentiment, n, fill = sentiment)) +
facet_wrap(~ author) +
coord_flip() +
theme(legend.position = "none")+
ggtitle("Top sentimental festure for each author")
#dev.off()
# Keep words that have been classified into the NRC lexicon.
nrc <- get_sentiments('nrc')
sentiments  <- inner_join(spooky_wrd, get_sentiments('nrc'), by = "word")
count(sentiments, sentiment)
count(sentiments, author, sentiment)
#png("../figs/Sentimental analysis.png")
ggplot(count(sentiments, sentiment)) +
geom_col(aes(sentiment, n, fill = sentiment))+
ggtitle("Sentimental features for the whole data")
#dev.off()
#png("../figs//Sentimental analysis for each author.png")
ggplot(count(sentiments, author, sentiment)) +
geom_col(aes(sentiment, n, fill = sentiment)) +
facet_wrap(~ author) +
coord_flip() +
theme(legend.position = "none")+
ggtitle("Top sentimental festure for each author")
#dev.off()
