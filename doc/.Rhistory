small_sample <- spooky[1:10,]
SScore<- unlist(lapply(small_sample$text, scoreSentence))
small_sample <- cbind(small_sample, SScore)
# If the score of sentence >0, it considers as the positive sentiment. If the score of sentence <0, it considers as the negative sentiment.
#png("../figs/Sentimental analysis at sentence level.png")
qplot( author,SScore, data=small_sample, geom=c("boxplot", "jitter"),
fill=author, main="Sentence Sentimental Scores for the first 100 Samples",
xlab="", ylab="Sentence Sentimental Score")
#dev.off()
# Use the dictionary nrc. Separate its sentiment columns into two categories: positive and negative
posWords <- c("anticipation","joy","positve","surprise","trust")
negWords <- c("anger","disgust","fear","negative","sadness")
# Set 1 for the positive category, and -1 for the negative category
SentimentJudge <-  for (i in 1:nrow(nrc)){
if (nrc$sentiment[i] %in% posWords) {
nrc$score[i] <- 1
}
else{
nrc$score[i] <- -1
}
}
nrc$lengths<-unlist(lapply(nrc$word, nchar))
nrc<-nrc[ order(-nrc[,4]),]
# Write the function to compare words between sentences and directory, and then calculate the score for each sentence
scoreSentence <- function(sentence){
score<-0
for(i in 1:nrow(nrc)){
count<- length(grep(nrc[i,1],sentence))
if(count){
score<-score + (count * nrc[i,3])
sentence<-sub(nrc[i,1],'',sentence)
}
}
score
}
# Pick a small set
small_sample <- spooky[1:100,]
SScore<- unlist(lapply(small_sample$text, scoreSentence))
small_sample <- cbind(small_sample, SScore)
# If the score of sentence >0, it considers as the positive sentiment. If the score of sentence <0, it considers as the negative sentiment.
#png("../figs/Sentimental analysis at sentence level.png")
qplot( author,SScore, data=small_sample, geom=c("boxplot", "jitter"),
fill=author, main="Sentence Sentimental Scores for the first 100 Samples",
xlab="", ylab="Sentence Sentimental Score")
#dev.off()
# Initialize the punctuation mark features
SPspooky <- spooky
SPspooky$comma      <- 0
SPspooky$semicolon  <- 0
SPspooky$quote      <- 0
# Convert string features into numerical features
SPspooky$comma[1:19579]        <- as.numeric(str_count(SPspooky$text[1:19579], ','))
SPspooky$semicolon[1:19579]    <- as.numeric(str_count(SPspooky$text[1:19579], ';'))
SPspooky$quote[1:19579]        <- as.numeric(str_count(SPspooky$text[1:19579], '"'))
SPspooky$sen_length <- str_length(SPspooky$text)
# Plot the length of sentences
p1 <- ggplot(SPspooky) +
geom_bar(aes(author, fill = author)) +
theme(legend.position = "left")
p2 <- ggplot(SPspooky) +
geom_density_ridges(aes(sen_length, author, fill = author)) +
scale_x_log10() +
theme(legend.position = "right") +
labs(x = "Sentence length [# characters]")
#png("../figs/Author words used and sentences length.png")
multiplot(p1, p2, cols =2)
#dev.off()
# Plot punctuation features for each author
p3 <- ggplot(SPspooky,aes(x= comma, fill = author, )) +
geom_density( bw = 0.1, alpha = 0.3) +
scale_x_log10() +
theme(legend.position = "top") +
labs(x = "Comma Used")
p4 <- ggplot(SPspooky) +
geom_density(aes(semicolon, fill = author), bw = 0.1, alpha = 0.3) +
scale_x_log10() +
theme(legend.position = "top") +
labs(x = "Semicolon Used")
p5 <- ggplot(SPspooky) +
geom_density(aes(quote, fill = author), bw = 0.1, alpha = 0.3) +
scale_x_log10() +
theme(legend.position="top")+
labs(x = "Quote Used")
#png("../figs/Punctuation marks frequences for each author.png")
multiplot(p3,p4,p5, cols =3)
#dev.off()
# Use PCA method to do classification
pca = prcomp(SPspooky[,4:7], scale. = TRUE)
p <- pca2d(pca, group=SPspooky$author, legend="bottomright",biplot=TRUE, biplot.vars=4, title = "PCA plot for punctuation marks and length of sentences")
#png("../figs/PCA Clastering Plot.png")
#pca2d(pca, group=SPspooky$author, legend="bottomright",biplot=TRUE, biplot.vars=4)
#dev.off()
# Use PCA method to do classification
pca = prcomp(SPspooky[,4:7], scale. = TRUE)
p <- pca2d(pca, group=SPspooky$author, legend="bottomright",biplot=TRUE, biplot.vars=4, title = PCA plot for punctuation marks and length of sentences)
# Use PCA method to do classification
pca = prcomp(SPspooky[,4:7], scale. = TRUE)
p <- pca2d(pca, group=SPspooky$author, legend="bottomright",biplot=TRUE, biplot.vars=4, title = 'PCA plot for punctuation marks and length of sentences')
#png("../figs/PCA Clastering Plot.png")
#pca2d(pca, group=SPspooky$author, legend="bottomright",biplot=TRUE, biplot.vars=4)
#dev.off()
# Initialize the punctuation mark features
SPspooky <- spooky
SPspooky$comma      <- 0
SPspooky$semicolon  <- 0
SPspooky$quote      <- 0
# Convert string features into numerical features
SPspooky$comma[1:19579]        <- as.numeric(str_count(SPspooky$text[1:19579], ','))
SPspooky$semicolon[1:19579]    <- as.numeric(str_count(SPspooky$text[1:19579], ';'))
SPspooky$quote[1:19579]        <- as.numeric(str_count(SPspooky$text[1:19579], '"'))
SPspooky$sen_length <- str_length(SPspooky$text)
# Plot the length of sentences
p1 <- ggplot(SPspooky) +
geom_bar(aes(author, fill = author)) +
theme(legend.position = "left")+
ggtitle("Words used for each author")
p2 <- ggplot(SPspooky) +
geom_density_ridges(aes(sen_length, author, fill = author)) +
scale_x_log10() +
theme(legend.position = "right") +
labs(x = "Sentence length [# characters]")+
ggtitle("Sentences length for each author")
#png("../figs/Author words used and sentences length.png")
multiplot(p1, p2, cols =2)
#dev.off()
# Plot punctuation features for each author
p3 <- ggplot(SPspooky,aes(x= comma, fill = author, )) +
geom_density( bw = 0.1, alpha = 0.3) +
scale_x_log10() +
theme(legend.position = "top") +
labs(x = "Comma Used")+
ggtitle("Comma used for each author")
p4 <- ggplot(SPspooky) +
geom_density(aes(semicolon, fill = author), bw = 0.1, alpha = 0.3) +
scale_x_log10() +
theme(legend.position = "top") +
labs(x = "Semicolon Used")+
ggtitle("Semicolon used for each author")
p5 <- ggplot(SPspooky) +
geom_density(aes(quote, fill = author), bw = 0.1, alpha = 0.3) +
scale_x_log10() +
theme(legend.position="top")+
labs(x = "Quote Used")+
ggtitle("Quote used for each author")
#png("../figs/Punctuation marks frequences for each author.png")
multiplot(p3,p4,p5, cols =3)
#dev.off()
# Keep words that have been classified into the NRC lexicon.
nrc <- get_sentiments('nrc')
sentiments  <- inner_join(spooky_wrd, get_sentiments('nrc'), by = "word")
count(sentiments, sentiment)
count(sentiments, author, sentiment)
#png("../figs/Sentimental analysis.png")
ggplot(count(sentiments, sentiment)) +
geom_col(aes(sentiment, n, fill = sentiment))+
ggtitle("Sentimental features for the whole data")
#dev.off()
#png("../figs//Sentimental analysis for each author.png")
ggplot(count(sentiments, author, sentiment)) +
geom_col(aes(sentiment, n, fill = sentiment)) +
facet_wrap(~ author) +
coord_flip() +
theme(legend.position = "none")+
ggtitle("Top sentimental festure for each author")
#dev.off()
# Keep words that have been classified into the NRC lexicon.
nrc <- get_sentiments('nrc')
sentiments  <- inner_join(spooky_wrd, get_sentiments('nrc'), by = "word")
count(sentiments, sentiment)
count(sentiments, author, sentiment)
#png("../figs/Sentimental analysis.png")
ggplot(count(sentiments, sentiment)) +
geom_col(aes(sentiment, n, fill = sentiment))+
ggtitle("Sentimental features for the whole data")
#dev.off()
#png("../figs//Sentimental analysis for each author.png")
ggplot(count(sentiments, author, sentiment)) +
geom_col(aes(sentiment, n, fill = sentiment)) +
facet_wrap(~ author) +
coord_flip() +
theme(legend.position = "none")+
ggtitle("Top sentimental festure for each author")
#dev.off()
# R package load
library(ggplot2)
library(dplyr)
library(tibble)
library(tidyr)
library(stringr)
library(tidytext)
library(topicmodels)
library(wordcloud)
library(ggridges)
library(textstem)
library(compare)
library(devtools)
library(pca3d)
library(wordcloud2)
# Reference: http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
require(grid)
# Make a list from the ... arguments and plotlist
plots <- c(list(...), plotlist)
numPlots = length(plots)
# If layout is NULL, then use 'cols' to determine layout
if (is.null(layout)) {
# Make the panel
# ncol: Number of columns of plots
# nrow: Number of rows needed, calculated from # of cols
layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
ncol = cols, nrow = ceiling(numPlots/cols))
}
if (numPlots==1) {
print(plots[[1]])
} else {
# Set up the page
grid.newpage()
pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
# Make each plot, in the correct location
for (i in 1:numPlots) {
# Get the i,j matrix positions of the regions that contain this subplot
matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
layout.pos.col = matchidx$col))
}
}
}
# Read the data
spooky <- read.csv('../data/spooky.csv', as.is = TRUE)
sum(is.na(spooky))
spooky$author <- as.factor(spooky$author)
# Make a table with one word per row and remove `stop words` (i.e. the common words).
spooky_wrd <- unnest_tokens(spooky, word, text)
spooky_wrd <- anti_join(spooky_wrd, stop_words, by = "word")
# Streaming
spooky_wrd1 <- spooky_wrd
spooky_wrd1$stem <- stem_words(spooky_wrd1$word, language = "porter")
# Use the dictionary nrc. Separate its sentiment columns into two categories: positive and negative
posWords <- c("anticipation","joy","positve","surprise","trust")
negWords <- c("anger","disgust","fear","negative","sadness")
# Set 1 for the positive category, and -1 for the negative category
SentimentJudge <-  for (i in 1:nrow(nrc)){
if (nrc$sentiment[i] %in% posWords) {
nrc$score[i] <- 1
}
else{
nrc$score[i] <- -1
}
}
# Keep words that have been classified into the NRC lexicon.
nrc <- get_sentiments('nrc')
sentiments  <- inner_join(spooky_wrd, get_sentiments('nrc'), by = "word")
count(sentiments, sentiment)
count(sentiments, author, sentiment)
#png("../figs/Sentimental analysis.png")
ggplot(count(sentiments, sentiment)) +
geom_col(aes(sentiment, n, fill = sentiment))+
ggtitle("Sentimental features for the whole data")
#dev.off()
#png("../figs//Sentimental analysis for each author.png")
ggplot(count(sentiments, author, sentiment)) +
geom_col(aes(sentiment, n, fill = sentiment)) +
facet_wrap(~ author) +
coord_flip() +
theme(legend.position = "none")+
ggtitle("Top sentimental festure for each author")
#dev.off()
# Use the dictionary nrc. Separate its sentiment columns into two categories: positive and negative
posWords <- c("anticipation","joy","positve","surprise","trust")
negWords <- c("anger","disgust","fear","negative","sadness")
# Set 1 for the positive category, and -1 for the negative category
SentimentJudge <-  for (i in 1:nrow(nrc)){
if (nrc$sentiment[i] %in% posWords) {
nrc$score[i] <- 1
}
else{
nrc$score[i] <- -1
}
}
nrc$lengths<-unlist(lapply(nrc$word, nchar))
nrc<-nrc[ order(-nrc[,4]),]
# Write the function to compare words between sentences and directory, and then calculate the score for each sentence
scoreSentence <- function(sentence){
score<-0
for(i in 1:nrow(nrc)){
count<- length(grep(nrc[i,1],sentence))
if(count){
score<-score + (count * nrc[i,3])
sentence<-sub(nrc[i,1],'',sentence)
}
}
score
}
# Pick a small set
small_sample <- spooky[1:100,]
SScore<- unlist(lapply(small_sample$text, scoreSentence))
# Use the dictionary nrc. Separate its sentiment columns into two categories: positive and negative
posWords <- c("anticipation","joy","positve","surprise","trust")
negWords <- c("anger","disgust","fear","negative","sadness")
# Set 1 for the positive category, and -1 for the negative category
SentimentJudge <-  for (i in 1:nrow(nrc)){
if (nrc$sentiment[i] %in% posWords) {
nrc$score[i] <- 1
}
else{
nrc$score[i] <- -1
}
}
nrc$lengths<-unlist(lapply(nrc$word, nchar))
nrc<-nrc[ order(-nrc[,4]),]
# Write the function to compare words between sentences and directory, and then calculate the score for each sentence
scoreSentence <- function(sentence){
score<-0
for(i in 1:nrow(nrc)){
count<- length(grep(nrc[i,1],sentence))
if(count){
score<-score + (count * nrc[i,3])
sentence<-sub(nrc[i,1],'',sentence)
}
}
score
}
# Pick a small set
small_sample <- spooky[500:800,]
SScore<- unlist(lapply(small_sample$text, scoreSentence))
small_sample <- cbind(small_sample, SScore)
# If the score of sentence >0, it considers as the positive sentiment. If the score of sentence <0, it considers as the negative sentiment.
#png("../figs/Sentimental analysis at sentence level.png")
qplot( author,SScore, data=small_sample, geom=c("boxplot", "jitter"),
fill=author, main="Sentence Sentimental Scores for the first 100 Samples",
xlab="", ylab="Sentence Sentimental Score")
#dev.off()
# Use the dictionary nrc. Separate its sentiment columns into two categories: positive and negative
posWords <- c("anticipation","joy","positve","surprise","trust")
negWords <- c("anger","disgust","fear","negative","sadness")
# Set 1 for the positive category, and -1 for the negative category
SentimentJudge <-  for (i in 1:nrow(nrc)){
if (nrc$sentiment[i] %in% posWords) {
nrc$score[i] <- 1
}
else{
nrc$score[i] <- -1
}
}
nrc$lengths<-unlist(lapply(nrc$word, nchar))
nrc<-nrc[ order(-nrc[,4]),]
# Write the function to compare words between sentences and directory, and then calculate the score for each sentence
scoreSentence <- function(sentence){
score<-0
for(i in 1:nrow(nrc)){
count<- length(grep(nrc[i,1],sentence))
if(count){
score<-score + (count * nrc[i,3])
sentence<-sub(nrc[i,1],'',sentence)
}
}
score
}
# Pick a small set
small_sample <- spooky[50:80,]
SScore<- unlist(lapply(small_sample$text, scoreSentence))
small_sample <- cbind(small_sample, SScore)
# If the score of sentence >0, it considers as the positive sentiment. If the score of sentence <0, it considers as the negative sentiment.
#png("../figs/Sentimental analysis at sentence level.png")
qplot( author,SScore, data=small_sample, geom=c("boxplot", "jitter"),
fill=author, main="Sentence Sentimental Scores for the first 100 Samples",
xlab="", ylab="Sentence Sentimental Score")
#dev.off()
# Use the dictionary nrc. Separate its sentiment columns into two categories: positive and negative
posWords <- c("anticipation","joy","positve","surprise","trust")
negWords <- c("anger","disgust","fear","negative","sadness")
# Set 1 for the positive category, and -1 for the negative category
SentimentJudge <-  for (i in 1:nrow(nrc)){
if (nrc$sentiment[i] %in% posWords) {
nrc$score[i] <- 1
}
else{
nrc$score[i] <- -1
}
}
nrc$lengths<-unlist(lapply(nrc$word, nchar))
nrc<-nrc[ order(-nrc[,4]),]
# Write the function to compare words between sentences and directory, and then calculate the score for each sentence
scoreSentence <- function(sentence){
score<-0
for(i in 1:nrow(nrc)){
count<- length(grep(nrc[i,1],sentence))
if(count){
score<-score + (count * nrc[i,3])
sentence<-sub(nrc[i,1],'',sentence)
}
}
score
}
# Pick a small set
small_sample <- spooky[50:150,]
SScore<- unlist(lapply(small_sample$text, scoreSentence))
small_sample <- cbind(small_sample, SScore)
# If the score of sentence >0, it considers as the positive sentiment. If the score of sentence <0, it considers as the negative sentiment.
#png("../figs/Sentimental analysis at sentence level.png")
qplot( author,SScore, data=small_sample, geom=c("boxplot", "jitter"),
fill=author, main="Sentence Sentimental Scores for the first 100 Samples",
xlab="", ylab="Sentence Sentimental Score")
#dev.off()
png("../figs/SentimentalAnalysisSentence.png")
qplot( author,SScore, data=small_sample, geom=c("boxplot", "jitter"),
fill=author, main="Sentence Sentimental Scores for the first 100 Samples",
xlab="", ylab="Sentence Sentimental Score")
png("../figs/SentimentalAnalysisSentence.png")
qplot( author,SScore, data=small_sample, geom=c("boxplot", "jitter"),
fill=author, main="Sentence Sentimental Scores for the first 100 Samples",
xlab="", ylab="Sentence Sentimental Score")
png("../figs/SentimentalAnalysisSentence.png")
qplot( author,SScore, data=small_sample, geom=c("boxplot", "jitter"),
fill=author, main="Sentence Sentimental Scores for the first 100 Samples",
xlab="", ylab="Sentence Sentimental Score")
dev.off()
# If the score of sentence >0, it considers as the positive sentiment. If the score of sentence <0, it considers as the negative sentiment.
#png("../figs/SentimentalAnalysisSentence.png")
qplot( author,SScore, data=small_sample, geom=c("boxplot", "jitter"),
fill=author, main="Sentence Sentimental Scores for the first 100 Samples",
xlab="", ylab="Sentence Sentimental Score")
# Use the dictionary nrc. Separate its sentiment columns into two categories: positive and negative
posWords <- c("anticipation","joy","positve","surprise","trust")
negWords <- c("anger","disgust","fear","negative","sadness")
# Set 1 for the positive category, and -1 for the negative category
SentimentJudge <-  for (i in 1:nrow(nrc)){
if (nrc$sentiment[i] %in% posWords) {
nrc$score[i] <- 1
}
else{
nrc$score[i] <- -1
}
}
nrc$lengths<-unlist(lapply(nrc$word, nchar))
nrc<-nrc[ order(-nrc[,4]),]
# Write the function to compare words between sentences and directory, and then calculate the score for each sentence
scoreSentence <- function(sentence){
score<-0
for(i in 1:nrow(nrc)){
count<- length(grep(nrc[i,1],sentence))
if(count){
score<-score + (count * nrc[i,3])
sentence<-sub(nrc[i,1],'',sentence)
}
}
score
}
# Pick a small set
small_sample <- spooky[50:150,]
SScore<- unlist(lapply(small_sample$text, scoreSentence))
small_sample <- cbind(small_sample, SScore)
# If the score of sentence >0, it considers as the positive sentiment. If the score of sentence <0, it considers as the negative sentiment.
#png("../figs/SentimentalAnalysisSentence.png")
qplot( author,SScore, data=small_sample, geom=c("boxplot", "jitter"),
fill=author, main="Sentence Sentimental Scores for the first 100 Samples",
xlab="", ylab="Sentence Sentimental Score")
#dev.off()
# Initialize the punctuation mark features
SPspooky <- spooky
SPspooky$comma      <- 0
SPspooky$semicolon  <- 0
SPspooky$quote      <- 0
# Convert string features into numerical features
SPspooky$comma[1:19579]        <- as.numeric(str_count(SPspooky$text[1:19579], ','))
SPspooky$semicolon[1:19579]    <- as.numeric(str_count(SPspooky$text[1:19579], ';'))
SPspooky$quote[1:19579]        <- as.numeric(str_count(SPspooky$text[1:19579], '"'))
SPspooky$sen_length <- str_length(SPspooky$text)
# Plot the length of sentences
p1 <- ggplot(SPspooky) +
geom_bar(aes(author, fill = author)) +
theme(legend.position = "left")+
ggtitle("Words used for each author")
p2 <- ggplot(SPspooky) +
geom_density_ridges(aes(sen_length, author, fill = author)) +
scale_x_log10() +
theme(legend.position = "right") +
labs(x = "Sentence length [# characters]")+
ggtitle("Sentences length for each author")
png("../figs/Author words used and sentences length.png")
multiplot(p1, p2, cols =2)
dev.off()
# Plot punctuation features for each author
p3 <- ggplot(SPspooky,aes(x= comma, fill = author, )) +
geom_density( bw = 0.1, alpha = 0.3) +
scale_x_log10() +
theme(legend.position = "top") +
labs(x = "Comma Used")+
ggtitle("Comma used for each author")
p4 <- ggplot(SPspooky) +
geom_density(aes(semicolon, fill = author), bw = 0.1, alpha = 0.3) +
scale_x_log10() +
theme(legend.position = "top") +
labs(x = "Semicolon Used")+
ggtitle("Semicolon used for each author")
p5 <- ggplot(SPspooky) +
geom_density(aes(quote, fill = author), bw = 0.1, alpha = 0.3) +
scale_x_log10() +
theme(legend.position="top")+
labs(x = "Quote Used")+
ggtitle("Quote used for each author")
png("../figs/PunctuationAuthor.png")
multiplot(p3,p4,p5, cols =3)
dev.off()
