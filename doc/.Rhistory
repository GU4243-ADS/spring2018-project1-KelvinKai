else{
nrc$score[i] <- -1
}
}
nrc$lengths<-unlist(lapply(nrc$word, nchar))
nrc<-nrc[ order(-nrc[,4]),]
scoreSentence <- function(sentence){
score<-0
for(i in 1:nrow(nrc)){
count<- length(grep(nrc[i,1],sentence))
if(count){
score<-score + (count * nrc[i,3])
sentence<-sub(nrc[i,1],'',sentence)
}
}
score
}
small_sample <- spooky[1:100,]
SScore<- unlist(lapply(small_sample$text, scoreSentence))
small_sample <- cbind(small_sample, SScore)
qplot( author,SScore, data=small_sample, geom=c("boxplot", "jitter"),
fill=author, main="Sentence Sentimental Scores for the first 100 Samples",
xlab="", ylab="Sentence Sentimental Score")
View(pca)
nrc
pca = prcomp(SPspooky[,4:7], scale. = TRUE)
p <- pca2d(pca, group=SPspooky$author, legend="bottomright",biplot=TRUE, biplot.vars=4)
png("../figs/PCA Clastering Plot.png")
p
dev.off()
pca = prcomp(SPspooky[,4:7], scale. = TRUE)
p <- pca2d(pca, group=SPspooky$author, legend="bottomright",biplot=TRUE, biplot.vars=4)
png("../figs/PCA Clastering Plot.png")
pca2d(pca, group=SPspooky$author, legend="bottomright",biplot=TRUE, biplot.vars=4)
dev.off()
#R package used in project
library(ggplot2)
library(dplyr)
library(tibble)
library(tidyr)
library(stringr)
library(tidytext)
library(topicmodels)
library(wordcloud)
library(ggridges)
library(textstem)
library(compare)
library(NMF)
library(devtools)
library(pca3d)
# Refer to http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
require(grid)
# Make a list from the ... arguments and plotlist
plots <- c(list(...), plotlist)
numPlots = length(plots)
# If layout is NULL, then use 'cols' to determine layout
if (is.null(layout)) {
# Make the panel
# ncol: Number of columns of plots
# nrow: Number of rows needed, calculated from # of cols
layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
ncol = cols, nrow = ceiling(numPlots/cols))
}
if (numPlots==1) {
print(plots[[1]])
} else {
# Set up the page
grid.newpage()
pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
# Make each plot, in the correct location
for (i in 1:numPlots) {
# Get the i,j matrix positions of the regions that contain this subplot
matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
layout.pos.col = matchidx$col))
}
}
}
# read the data
spooky <- read.csv('../data/spooky.csv', as.is = TRUE)
sum(is.na(spooky))
spooky$author <- as.factor(spooky$author)
# Make a table with one word per row and remove `stop words` (i.e. the common words).
spooky_wrd <- unnest_tokens(spooky, word, text)
spooky_wrd <- anti_join(spooky_wrd, stop_words, by = "word")
#Stream words
spooky_wrd1 <- spooky_wrd
spooky_wrd1$stem <- stem_words(spooky_wrd1$word, language = "porter")
# set n=2 to split words, so two words are considered as one cell in the data set.
spooky_wrd2 <- unnest_tokens(spooky, word, text, token = "skip_ngrams", n = 2)
spooky_wrd2 <- anti_join(spooky_wrd2, stop_words, by = "word")
# set n=2
spooky_wrd3 <- unnest_tokens(spooky, word, text, token = "skip_ngrams", n = 3)
spooky_wrd3 <- anti_join(spooky_wrd3, stop_words, by = "word")
# explore words without streaming frequency
x <- count(group_by(spooky_wrd, word))
words <- x$word
freqs <- x$n
png("../figs/Wordcloud_all.png")
wordcloud(words, freqs, max.words = 50, color = c("purple4", "red4", "black"))
dev.off()
# explore words with streaming frequency
x1 <- count(group_by(spooky_wrd1, stem))
words1 <- x1$stem
freqs1 <- x1$n
#png("../figs/Wordcloud_streamingWords.png")
wordcloud(words1, freqs1, max.words = 50, color = c("purple4", "red4", "black"))
#dev.off()
# explore words without streaming frequency
x <- count(group_by(spooky_wrd, word))
words <- x$word
freqs <- x$n
png("../figs/Wordcloud_all.png")
wordcloud(words, freqs, max.words = 50, color = c("purple4", "red4", "black"))
dev.off()
# explore words with streaming frequency
x1 <- count(group_by(spooky_wrd1, stem))
words1 <- x1$stem
freqs1 <- x1$n
#png("../figs/Wordcloud_streamingWords.png")
wordcloud(words1, freqs1, max.words = 50, color = c("purple4", "red4", "black"))
#dev.off()
x3 <- x[order(-x$n),]
x4 <- x1[order(-x1$n),]
# compare the top 50 frequencies words between streaming and unstreaming data sets
comparison <- cbind(x3$word[1:50], x3$n[1:50], x4$stem[1:50], x4$n[1:50])
comparison <- data.frame(comparison)
names(comparison) <- c("word non-streaming","frequency1", "word streaming", "frequency2")
comparison
# Counts number of times each author used each word.
author_words <- count(group_by(spooky_wrd1, stem, author))
# Counts number of times each word was used.
all_words    <- rename(count(group_by(spooky_wrd1, stem)), all = n)
author_words <- left_join(author_words, all_words, by = "stem")
author_words <- arrange(author_words, desc(all))
author_words <- ungroup(head(author_words, 81))
f1 <- ggplot(author_words) +
geom_col(aes(reorder(stem, all, FUN = min), n, fill = author)) +
xlab(NULL) +
coord_flip() +
facet_wrap(~ author) +
theme(legend.position = "none")+
ggtitle("Most frequent words used by Author (streaming)")
# Counts number of times each author used each word.
author_words <- count(group_by(spooky_wrd, word, author))
# Counts number of times each word was used.
all_words    <- rename(count(group_by(spooky_wrd, word)), all = n)
author_words <- left_join(author_words, all_words, by = "word")
author_words <- arrange(author_words, desc(all))
author_words <- ungroup(head(author_words, 81))
f2 <- ggplot(author_words) +
geom_col(aes(reorder(word, all, FUN = min), n, fill = author)) +
xlab(NULL) +
coord_flip() +
facet_wrap(~ author) +
theme(legend.position = "none")+
ggtitle("Most frequent words used by Author (non-streaming)")
png("../figs/Most frequent words used by Author (non-streaming).png")
f2
dev.off()
png("../figs/Most frequent words used by Author (streaming).png")
f1
dev.off()
SPspooky <- spooky
SPspooky$comma      <- 0
SPspooky$semicolon  <- 0
SPspooky$quote      <- 0
SPspooky$comma[1:19579]        <- as.numeric(str_count(SPspooky$text[1:19579], ','))
SPspooky$semicolon[1:19579]    <- as.numeric(str_count(SPspooky$text[1:19579], ';'))
SPspooky$quote[1:19579]        <- as.numeric(str_count(SPspooky$text[1:19579], '"'))
SPspooky$sen_length <- str_length(SPspooky$text)
p1 <- ggplot(SPspooky) +
geom_bar(aes(author, fill = author)) +
theme(legend.position = "none")
p2 <- ggplot(SPspooky) +
geom_density_ridges(aes(sen_length, author, fill = author)) +
scale_x_log10() +
theme(legend.position = "none") +
labs(x = "Sentence length [# characters]")
png("../figs/Author words used and sentences length.png")
multiplot(p1, p2, cols =2)
dev.off()
p3 <- ggplot(SPspooky,aes(x= comma, fill = author, )) +
geom_density( bw = 0.1, alpha = 0.3) +
scale_x_log10() +
theme(legend.position = "none") +
labs(x = "Comma Used")
p4 <- ggplot(SPspooky) +
geom_density(aes(semicolon, fill = author), bw = 0.1, alpha = 0.3) +
scale_x_log10() +
theme(legend.position = "none") +
labs(x = "Semicolon Used")
p5 <- ggplot(SPspooky) +
geom_density(aes(quote, fill = author), bw = 0.1, alpha = 0.3) +
scale_x_log10() +
theme(legend.position = "none") +
labs(x = "Quote Used")
png("../figs/Punctuation marks frequences for each author.png")
multiplot(p3,p4,p5, cols =3)
dev.off()
# Refer to http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
require(grid)
# Make a list from the ... arguments and plotlist
plots <- c(list(...), plotlist)
numPlots = length(plots)
# If layout is NULL, then use 'cols' to determine layout
if (is.null(layout)) {
# Make the panel
# ncol: Number of columns of plots
# nrow: Number of rows needed, calculated from # of cols
layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
ncol = cols, nrow = ceiling(numPlots/cols))
}
if (numPlots==1) {
print(plots[[1]])
} else {
# Set up the page
grid.newpage()
pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
# Make each plot, in the correct location
for (i in 1:numPlots) {
# Get the i,j matrix positions of the regions that contain this subplot
matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
layout.pos.col = matchidx$col))
}
}
}
# Counts number of times each author used each word.
author_words <- count(group_by(spooky_wrd1, stem, author))
# Counts number of times each word was used.
all_words    <- rename(count(group_by(spooky_wrd1, stem)), all = n)
author_words <- left_join(author_words, all_words, by = "stem")
author_words <- arrange(author_words, desc(all))
author_words <- ungroup(head(author_words, 81))
f1 <- ggplot(author_words) +
geom_col(aes(reorder(stem, all, FUN = min), n, fill = author)) +
xlab(NULL) +
coord_flip() +
facet_wrap(~ author) +
theme(legend.position = "none")+
ggtitle("Most frequent words used by Author (streaming)")
# Counts number of times each author used each word.
author_words <- count(group_by(spooky_wrd, word, author))
# Counts number of times each word was used.
all_words    <- rename(count(group_by(spooky_wrd, word)), all = n)
author_words <- left_join(author_words, all_words, by = "word")
author_words <- arrange(author_words, desc(all))
author_words <- ungroup(head(author_words, 81))
f2 <- ggplot(author_words) +
geom_col(aes(reorder(word, all, FUN = min), n, fill = author)) +
xlab(NULL) +
coord_flip() +
facet_wrap(~ author) +
theme(legend.position = "none")+
ggtitle("Most frequent words used by Author (non-streaming)")
png("../figs/Most frequent words used by Author (non-streaming).png")
f2
dev.off()
png("../figs/Most frequent words used by Author (streaming).png")
f1
dev.off()
SPspooky <- spooky
SPspooky$comma      <- 0
SPspooky$semicolon  <- 0
SPspooky$quote      <- 0
SPspooky$comma[1:19579]        <- as.numeric(str_count(SPspooky$text[1:19579], ','))
SPspooky$semicolon[1:19579]    <- as.numeric(str_count(SPspooky$text[1:19579], ';'))
SPspooky$quote[1:19579]        <- as.numeric(str_count(SPspooky$text[1:19579], '"'))
SPspooky$sen_length <- str_length(SPspooky$text)
p1 <- ggplot(SPspooky) +
geom_bar(aes(author, fill = author)) +
theme(legend.position = "none")
p2 <- ggplot(SPspooky) +
geom_density_ridges(aes(sen_length, author, fill = author)) +
scale_x_log10() +
theme(legend.position = "none") +
labs(x = "Sentence length [# characters]")
png("../figs/Author words used and sentences length.png")
multiplot(p1, p2, cols =2)
dev.off()
p3 <- ggplot(SPspooky,aes(x= comma, fill = author, )) +
geom_density( bw = 0.1, alpha = 0.3) +
scale_x_log10() +
theme(legend.position = "none") +
labs(x = "Comma Used")
p4 <- ggplot(SPspooky) +
geom_density(aes(semicolon, fill = author), bw = 0.1, alpha = 0.3) +
scale_x_log10() +
theme(legend.position = "none") +
labs(x = "Semicolon Used")
p5 <- ggplot(SPspooky) +
geom_density(aes(quote, fill = author), bw = 0.1, alpha = 0.3) +
scale_x_log10() +
theme(legend.position = "none") +
labs(x = "Quote Used")
png("../figs/Punctuation marks frequences for each author.png")
multiplot(p3,p4,p5, cols =3)
dev.off()
pca = prcomp(SPspooky[,4:7], scale. = TRUE)
p <- pca2d(pca, group=SPspooky$author, legend="bottomright",biplot=TRUE, biplot.vars=4)
png("../figs/PCA Clastering Plot.png")
pca2d(pca, group=SPspooky$author, legend="bottomright",biplot=TRUE, biplot.vars=4)
dev.off()
# Keep words that have been classified within the NRC lexicon.
nrc <- get_sentiments('nrc')
sentiments  <- inner_join(spooky_wrd, get_sentiments('nrc'), by = "word")
count(sentiments, sentiment)
count(sentiments, author, sentiment)
png("../figs/Sentimental analysis.png")
ggplot(count(sentiments, sentiment)) +
geom_col(aes(sentiment, n, fill = sentiment))
dev.off()
png("../figs//Sentimental analysis for each author.png")
ggplot(count(sentiments, author, sentiment)) +
geom_col(aes(sentiment, n, fill = sentiment)) +
facet_wrap(~ author) +
coord_flip() +
theme(legend.position = "none")
dev.off()
#use the dictionary nrc. Seperate its sentiment columns into two categories: positive and negative
posWords <- c("anticipation","joy","positve","surprise","trust")
negWords <- c("anger","disgust","fear","negative","sadness")
#set 1 for positive category, and -1 for negative category.
SentimentJudge <-  for (i in 1:nrow(nrc)){
if (nrc$sentiment[i] %in% posWords) {
nrc$score[i] <- 1
}
else{
nrc$score[i] <- -1
}
}
nrc$lengths<-unlist(lapply(nrc$word, nchar))
nrc<-nrc[ order(-nrc[,4]),]
#write the function to compare words between sentences and dictory, and then calculate the socre for each sentence
scoreSentence <- function(sentence){
score<-0
for(i in 1:nrow(nrc)){
count<- length(grep(nrc[i,1],sentence))
if(count){
score<-score + (count * nrc[i,3])
sentence<-sub(nrc[i,1],'',sentence)
}
}
score
}
#pick a small set
small_sample <- spooky[1:100,]
SScore<- unlist(lapply(small_sample$text, scoreSentence))
small_sample <- cbind(small_sample, SScore)
#If the score of sentence >0, it consider as the positive sentiment. If the score of sentence <0, it consider as the negative sentiment.
png("../figs/Sentimental analysis at sentence level.png")
qplot( author,SScore, data=small_sample, geom=c("boxplot", "jitter"),
fill=author, main="Sentence Sentimental Scores for the first 100 Samples",
xlab="", ylab="Sentence Sentimental Score")
dev.off()
#R package used in project
library(ggplot2)
library(dplyr)
library(tibble)
library(tidyr)
library(stringr)
library(tidytext)
library(topicmodels)
library(wordcloud)
library(ggridges)
library(textstem)
library(compare)
library(NMF)
library(devtools)
library(pca3d)
library(wordcloud2)
# Reference: http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
require(grid)
# Make a list from the ... arguments and plotlist
plots <- c(list(...), plotlist)
numPlots = length(plots)
# If layout is NULL, then use 'cols' to determine layout
if (is.null(layout)) {
# Make the panel
# ncol: Number of columns of plots
# nrow: Number of rows needed, calculated from # of cols
layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
ncol = cols, nrow = ceiling(numPlots/cols))
}
if (numPlots==1) {
print(plots[[1]])
} else {
# Set up the page
grid.newpage()
pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
# Make each plot, in the correct location
for (i in 1:numPlots) {
# Get the i,j matrix positions of the regions that contain this subplot
matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
layout.pos.col = matchidx$col))
}
}
}
# read the data
spooky <- read.csv('../data/spooky.csv', as.is = TRUE)
sum(is.na(spooky))
spooky$author <- as.factor(spooky$author)
# Make a table with one word per row and remove `stop words` (i.e. the common words).
spooky_wrd <- unnest_tokens(spooky, word, text)
spooky_wrd <- anti_join(spooky_wrd, stop_words, by = "word")
#Stream words
spooky_wrd1 <- spooky_wrd
spooky_wrd1$stem <- stem_words(spooky_wrd1$word, language = "porter")
# set n=2 to split words, so two words are considered as one cell in the data set.
spooky_wrd2 <- unnest_tokens(spooky, word, text, token = "skip_ngrams", n = 2)
spooky_wrd2 <- anti_join(spooky_wrd2, stop_words, by = "word")
# set n=2
spooky_wrd3 <- unnest_tokens(spooky, word, text, token = "skip_ngrams", n = 3)
spooky_wrd3 <- anti_join(spooky_wrd3, stop_words, by = "word")
# explore words without streaming frequency
x <- count(group_by(spooky_wrd, word))
words <- x$word
freqs <- x$n
png("../figs/Wordcloud_all.png")
wordcloud(words, freqs, max.words = 50, color = c("purple4", "red4", "black"))
dev.off()
# explore words with streaming frequency
x1 <- count(group_by(spooky_wrd1, stem))
words1 <- x1$stem
freqs1 <- x1$n
#png("../figs/Wordcloud_streamingWords.png")
wordcloud(words1, freqs1, max.words = 50, color = c("purple4", "red4", "black"))
#dev.off()
wordc1 <- data.frame(nbind(words1,freqs1))
wordcloud2(x1, figPath = "twitter.jpg")
figPath = system.file("../figs/examplest.png",package = "wordcloud2")
wordcloud2(x1, figPath = "twitter.jpg")
figPath = system.file("../figs/examplest.png",package = "wordcloud2")
wordcloud2(x1, figPath = "twitter.jpg")
wordcloud2(x1, figPath = figPath)
figPath = system.file("../figs/examplest.png",package = "wordcloud2")
wordcloud2(x1, figPath = figPath)
wordcloud2(x1, figPath = figPath, size= 1.5)
figPath = system.file("../figs/examplest.png",package = "wordcloud2")
figPath = system.file('../figs/examplest.png' ,package = "wordcloud2")
wordcloud2(x1, figPath = figPath, size= 1.5)
figPath = system.file('../figs/examplest.png' ,package = "wordcloud2")
wordcloud2(x1, figPath = figPath, size= 1.5)
figPath = system.file('figs/examplest.png' ,package = "wordcloud2")
wordcloud2(x1, figPath = figPath, size= 1.5)
wordcloud2(x1, size= 1.5, shape = "star")
wordcloud2(x1[1:50], size= 1.5, shape = "star")
wordcloud2(x1[1:50,], size= 1.5, shape = "star")
View(x1)
View(x1)
x1 <- x1[order(-x1$n),]
wordcloud2(x1[1:50,], size= 1.5, shape = "star")
wordcloud2(x1[1:50,], size= , shape = "star")
figPath = system.file('figs/examplest.png' ,package = "wordcloud2")
wordcloud2(x1[1:50,], figPath = figPath, size= , shape = "star")
figPath = system.file('examplest.png' ,package = "wordcloud2")
wordcloud2(x1[1:50,], figPath = figPath, size= , shape = "star")
wordcloud2(x1[1:50,], figPath = figPath, size= 1.5)
figPath = system.file('../doc/examplest.png' ,package = "wordcloud2")
wordcloud2(x1[1:50,], figPath = figPath, size= 1.5)
letterCloud( x1[1:150,], word = "R", color='random-light' , backgroundColor="black")
wordcloud2(x1[1:150,], size= 1.5, shape = star)
wordcloud2(x1[1:150,], size= 1.5, shape = "star")
wordcloud2(x1[1:150,], size= 1.0, shape = "star")
wordcloud2(x1[1:150,], size= 1.0, shape = "round")
# explore words without streaming frequency
x <- count(group_by(spooky_wrd, word))
words <- x$word
freqs <- x$n
x <- x[order(-x$n)]
# explore words without streaming frequency
x <- count(group_by(spooky_wrd, word))
words <- x$word
freqs <- x$n
x <- x[order(-x$n),]
png("../figs/Wordcloud_all.png")
wordcloud2(x[1:150,], size= 1.0, shape = "round")
dev.off()
wordcloud2(x[1:150,], size= 1.0, shape = "round")
png("../figs/Wordcloud_all.png")
wordcloud2(x[1:150,], size= 1.0, shape = "round")
dev.off()
wordcloud2(x[1:150,], size= 1.0, shape = "round")
